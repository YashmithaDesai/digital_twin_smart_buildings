#!/usr/bin/env python3
"""
Seed InfluxDB with ASHRAE deployment data and simulate IoT streams.

This script reads the processed dataset generated by `scripts/fetch_dataset.py`
and writes the metrics into InfluxDB so that the backend can query real data.

Usage examples:

    # Seed entire history into InfluxDB
    python database/timeseries/influx_init.py --mode seed

    # Stream data in "real time" (1 record/second) to mimic IoT sensors
    python database/timeseries/influx_init.py --mode stream --speed 1.0
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import Dict, Iterable, List

import numpy as np
import pandas as pd
from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS

# Workspace paths
PROJECT_ROOT = Path(__file__).resolve().parents[2]
DATA_DIR = PROJECT_ROOT / "data" / "processed"
DEPLOYMENT_DATA = DATA_DIR / "deployment_data.csv"
BUILDING_INFO = DATA_DIR / "building_info.json"

# Simulated zone configuration to create richer telemetry
ZONE_PROFILES: Dict[str, Dict[str, float]] = {
    "zone-east": {"energy_ratio": 0.4, "temp_bias": 0.6, "occ_bias": 0.05},
    "zone-west": {"energy_ratio": 0.35, "temp_bias": -0.4, "occ_bias": -0.02},
    "zone-core": {"energy_ratio": 0.25, "temp_bias": 0.0, "occ_bias": 0.01},
}


def require_dataset() -> pd.DataFrame:
    """Ensure processed dataset exists and load it."""
    if not DEPLOYMENT_DATA.exists():
        raise FileNotFoundError(
            "deployment_data.csv not found. "
            "Run `python scripts/fetch_dataset.py` first."
        )

    df = pd.read_csv(DEPLOYMENT_DATA, parse_dates=["timestamp"])
    if df.empty:
        raise ValueError("deployment_data.csv is empty.")

    return df.sort_values("timestamp").reset_index(drop=True)


def load_building_info() -> Dict:
    """Load metadata about the selected building."""
    if BUILDING_INFO.exists():
        with open(BUILDING_INFO) as fp:
            info = json.load(fp)
            info["building_id"] = str(info.get("building_id", "demo-building"))
            return info

    # Fallback metadata
    return {
        "building_id": "demo-building",
        "primary_use": "Office",
        "square_feet": 50000,
        "site_id": 0,
    }


def get_influx_client():
    """Create InfluxDB client and ensure the bucket exists."""
    influx_url = os.getenv("INFLUXDB_URL", "http://localhost:8086")
    influx_token = os.getenv("INFLUXDB_TOKEN")
    influx_org = os.getenv("INFLUXDB_ORG", "digital-twin")
    influx_bucket = os.getenv("INFLUXDB_BUCKET", "building_telemetry")

    if not influx_token:
        raise EnvironmentError(
            "INFLUXDB_TOKEN is not set. "
            "Create an API token in InfluxDB and export it before running."
        )

    client = InfluxDBClient(url=influx_url, token=influx_token, org=influx_org)
    buckets_api = client.buckets_api()
    bucket = buckets_api.find_bucket_by_name(influx_bucket)
    if bucket is None:
        org = client.organizations_api().find_organization(influx_org)
        buckets_api.create_bucket(bucket_name=influx_bucket, org_id=org.id)
        print(f"Created bucket '{influx_bucket}' in org '{influx_org}'.")

    return client, influx_bucket, influx_org


def build_points(row: pd.Series, building_id: str) -> List[Point]:
    """Convert one timestamp row into multiple zone metrics."""
    timestamp = row["timestamp"].to_pydatetime()
    energy = max(row.get("energy", 0.0), 0.0)
    temperature = row.get("temperature")
    humidity = row.get("humidity")
    occupancy = np.clip(row.get("occupancy", 0.3), 0.0, 1.0)

    points: List[Point] = []
    for zone, profile in ZONE_PROFILES.items():
        e_val = energy * profile["energy_ratio"]
        t_val = None if pd.isna(temperature) else temperature + profile["temp_bias"]
        h_val = None if pd.isna(humidity) else humidity
        o_val = np.clip(occupancy + profile["occ_bias"], 0.0, 1.0)

        points.append(
            Point("energy")
            .tag("building_id", building_id)
            .tag("zone_id", zone)
            .field("value", float(e_val))
            .time(timestamp, WritePrecision.NS)
        )

        if t_val is not None:
            points.append(
                Point("temperature")
                .tag("building_id", building_id)
                .tag("zone_id", zone)
                .field("value", float(t_val))
                .time(timestamp, WritePrecision.NS)
            )

        if h_val is not None:
            points.append(
                Point("humidity")
                .tag("building_id", building_id)
                .tag("zone_id", zone)
                .field("value", float(h_val))
                .time(timestamp, WritePrecision.NS)
            )

        points.append(
            Point("occupancy")
            .tag("building_id", building_id)
            .tag("zone_id", zone)
            .field("value", float(o_val))
            .time(timestamp, WritePrecision.NS)
        )

    return points


def seed_historical_data(df: pd.DataFrame, write_api, bucket: str, org: str, building_id: str, limit: int | None = None) -> None:
    """Bulk insert historical data into InfluxDB."""
    total_rows = len(df) if limit is None else min(limit, len(df))
    print(f"Seeding {total_rows} rows into InfluxDB...")

    batch: List[Point] = []
    batch_size = 5_000

    for idx, row in df.head(total_rows).iterrows():
        batch.extend(build_points(row, building_id))
        if len(batch) >= batch_size:
            write_api.write(bucket=bucket, org=org, record=batch)
            batch = []
            if idx % 1000 == 0:
                print(f"  wrote {idx + 1} rows...")

    if batch:
        write_api.write(bucket=bucket, org=org, record=batch)

    print("Historical seed complete.")


def stream_data(df: pd.DataFrame, write_api, bucket: str, org: str, building_id: str, speed: float, loop: bool) -> None:
    """Continuously stream data points to InfluxDB to mimic IoT telemetry."""
    print(f"Streaming data at {speed}s intervals (loop={loop})...")
    while True:
        for row in df.itertuples(index=False):
            points = build_points(pd.Series(row._asdict()), building_id)
            write_api.write(bucket=bucket, org=org, record=points)
            time.sleep(max(speed, 0.01))
        if not loop:
            break
    print("Streaming finished.")


def parse_args():
    parser = argparse.ArgumentParser(description="Seed InfluxDB with building telemetry.")
    parser.add_argument(
        "--mode",
        choices=["seed", "stream"],
        default="seed",
        help="Seed entire dataset or simulate live streaming.",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Limit number of rows when seeding (for quick tests).",
    )
    parser.add_argument(
        "--speed",
        type=float,
        default=1.0,
        help="Seconds between points when streaming.",
    )
    parser.add_argument(
        "--loop",
        action="store_true",
        help="When streaming, loop over the dataset indefinitely.",
    )
    return parser.parse_args()


def main():
    args = parse_args()
    df = require_dataset()
    building_info = load_building_info()
    building_id = building_info["building_id"]

    client, bucket, org = get_influx_client()
    write_api = client.write_api(write_options=SYNCHRONOUS)

    if args.mode == "seed":
        seed_historical_data(df, write_api, bucket, org, building_id, limit=args.limit)
    else:
        stream_data(df, write_api, bucket, org, building_id, speed=args.speed, loop=args.loop)

    client.close()


if __name__ == "__main__":
    try:
        main()
    except Exception as exc:
        print(f"[ERROR] {exc}")
        sys.exit(1)
